# ðŸ“š Phase 2: Application Code - Complete Walkthrough Guide

**Date Prepared:** December 9, 2025  
**For Session:** December 10, 2025  
**Duration:** ~2 hours (broken into manageable steps)

---

## ðŸŽ¯ Overview

Tomorrow we'll build **4 production-grade microservices** from scratch. This guide explains every file, every line of code, and the reasoning behind each decision.

---

## ðŸ“‹ What We'll Build Tomorrow

### **Service 1: task-service (FastAPI)** â­ PRIMARY FOCUS
- **Lines of Code:** ~800 lines
- **Files:** 15+ files
- **Time:** 45 minutes
- **Complexity:** Medium

### **Service 2: user-service (Flask)**
- **Lines of Code:** ~500 lines  
- **Files:** 10+ files
- **Time:** 30 minutes
- **Complexity:** Low-Medium

### **Service 3: auth-service (FastAPI + JWT)**
- **Lines of Code:** ~600 lines
- **Files:** 12+ files  
- **Time:** 30 minutes
- **Complexity:** Medium-High

### **Service 4: notification-service (AWS Lambda)**
- **Lines of Code:** ~200 lines
- **Files:** 3 files
- **Time:** 15 minutes
- **Complexity:** Low

---

## ðŸ—ï¸ Step-by-Step Build Plan for Tomorrow

---

## **PART 1: task-service (FastAPI) - 45 minutes**

### **Step 2.1: Directory Structure (5 minutes)**

```
app/services/task-service/
â”œâ”€â”€ src/                           # Source code (NOT app/)
â”‚   â”œâ”€â”€ __init__.py               # Makes 'src' a Python package
â”‚   â”œâ”€â”€ main.py                   # ðŸ”¥ APPLICATION ENTRY POINT
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                      # API Layer (HTTP endpoints)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes.py             # ðŸ”¥ CRUD endpoints
â”‚   â”‚   â””â”€â”€ dependencies.py       # Dependency injection
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                   # Data Models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ task.py               # ðŸ”¥ Pydantic models (validation)
â”‚   â”‚   â””â”€â”€ database.py           # ðŸ”¥ SQLAlchemy models (DB schema)
â”‚   â”‚
â”‚   â”œâ”€â”€ database/                 # Database Layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ connection.py         # ðŸ”¥ DB connection setup
â”‚   â”‚   â””â”€â”€ session.py            # Session management
â”‚   â”‚
â”‚   â”œâ”€â”€ observability/            # OpenTelemetry (Monitoring)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tracing.py            # ðŸ”¥ Distributed tracing
â”‚   â”‚   â”œâ”€â”€ metrics.py            # Prometheus metrics
â”‚   â”‚   â””â”€â”€ logging.py            # Structured logging
â”‚   â”‚
â”‚   â””â”€â”€ config/                   # Configuration
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ settings.py           # ðŸ”¥ Environment variables
â”‚
â”œâ”€â”€ tests/                        # Test Suite (>85% coverage)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py               # ðŸ”¥ Pytest fixtures (shared setup)
â”‚   â”‚
â”‚   â”œâ”€â”€ unit/                     # Unit Tests (fast, isolated)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_models.py        # Test Pydantic models
â”‚   â”‚   â””â”€â”€ test_routes.py        # Test API logic
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/              # Integration Tests (with DB)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ test_database.py      # Test DB operations
â”‚   â”‚
â”‚   â””â”€â”€ e2e/                      # End-to-End Tests (full API)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ test_api_e2e.py       # Complete workflows
â”‚
â”œâ”€â”€ Dockerfile                    # ðŸ”¥ Multi-stage Docker build
â”œâ”€â”€ .dockerignore                 # Exclude files from image
â”œâ”€â”€ requirements.txt              # ðŸ”¥ Python dependencies
â”œâ”€â”€ requirements-dev.txt          # Development dependencies
â”œâ”€â”€ pyproject.toml               # ðŸ”¥ Project config (pytest, black, etc.)
â”œâ”€â”€ .env.example                 # Environment variables template
â””â”€â”€ README.md                    # Service documentation
```

**ðŸ”¥ = Files we'll create tomorrow**

---

### **Step 2.2: Understanding the Flow (10 minutes)**

#### **Request Flow Diagram:**
```
Client Request
    â†“
[1] main.py (FastAPI app)
    â†“
[2] routes.py (API endpoint handler)
    â†“
[3] dependencies.py (Get database session)
    â†“
[4] database.py (SQLAlchemy query)
    â†“
[5] PostgreSQL Database
    â†“
[6] task.py (Pydantic response model)
    â†“
[7] Response to Client
```

**Parallel:** OpenTelemetry captures trace at each step âœ¨

---

### **Step 2.3: Core Files Explained**

---

#### **FILE 1: `src/main.py` - Application Entry Point**

**Purpose:** Bootstraps the FastAPI application

**What it does:**
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# Create FastAPI app
app = FastAPI(
    title="Task Service API",
    description="Production-grade task management microservice",
    version="1.0.0",
    docs_url="/docs",      # Swagger UI at /docs
    redoc_url="/redoc"     # ReDoc at /redoc
)

# Add CORS (Cross-Origin Resource Sharing)
# Allows frontend apps to call this API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],   # In production: specific domains only
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Import routes
from src.api import routes
app.include_router(routes.router, prefix="/api/v1")

# Health check endpoint
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "task-service",
        "version": "1.0.0"
    }

# Instrument with OpenTelemetry (automatic tracing)
FastAPIInstrumentor.instrument_app(app)
```

**Key Concepts:**
- âœ… `FastAPI()`: Creates the application instance
- âœ… `CORSMiddleware`: Enables cross-origin requests
- âœ… `include_router()`: Modular route organization
- âœ… `FastAPIInstrumentor`: Automatic distributed tracing
- âœ… `/docs`: Auto-generated API documentation

**Why these choices?**
- FastAPI auto-generates OpenAPI docs
- CORS needed for frontend integration
- Versioned API (`/api/v1`) for backward compatibility
- Health check for Kubernetes liveness probes

---

#### **FILE 2: `src/models/task.py` - Pydantic Models**

**Purpose:** Request/response validation and serialization

**What it does:**
```python
from pydantic import BaseModel, Field, validator
from typing import Optional
from datetime import datetime
from enum import Enum

# Enums for status and priority
class TaskStatus(str, Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    CANCELLED = "cancelled"

class TaskPriority(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

# Base model (shared fields)
class TaskBase(BaseModel):
    title: str = Field(..., min_length=1, max_length=200)
    description: Optional[str] = Field(None, max_length=2000)
    status: TaskStatus = TaskStatus.PENDING
    priority: TaskPriority = TaskPriority.MEDIUM
    
    @validator('title')
    def title_must_not_be_empty(cls, v):
        if not v or v.strip() == "":
            raise ValueError('Title cannot be empty')
        return v.strip()

# Request model (when creating task)
class TaskCreate(TaskBase):
    pass

# Request model (when updating task)
class TaskUpdate(BaseModel):
    title: Optional[str] = Field(None, min_length=1, max_length=200)
    description: Optional[str] = None
    status: Optional[TaskStatus] = None
    priority: Optional[TaskPriority] = None

# Response model (what API returns)
class TaskResponse(TaskBase):
    id: int
    created_at: datetime
    updated_at: Optional[datetime] = None
    user_id: Optional[int] = None
    
    class Config:
        orm_mode = True  # Allows creation from SQLAlchemy models
```

**Key Concepts:**
- âœ… `BaseModel`: Pydantic base class for validation
- âœ… `Field()`: Adds constraints (min_length, max_length)
- âœ… `Enum`: Restricts values to predefined options
- âœ… `@validator`: Custom validation logic
- âœ… `orm_mode`: Converts database models to Pydantic models

**Why Pydantic?**
- Automatic validation (catches bad data)
- Type safety (prevents bugs)
- Auto-generates JSON schema for docs
- No invalid data reaches your database

---

#### **FILE 3: `src/models/database.py` - SQLAlchemy Models**

**Purpose:** Database schema definition (tables, columns, relationships)

**What it does:**
```python
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey
from sqlalchemy.orm import declarative_base, relationship
from datetime import datetime

Base = declarative_base()

class Task(Base):
    """
    Task table schema in PostgreSQL
    
    Maps to table: tasks
    """
    __tablename__ = "tasks"
    
    # Primary Key
    id = Column(Integer, primary_key=True, index=True)
    
    # Task fields
    title = Column(String(200), nullable=False, index=True)
    description = Column(Text, nullable=True)
    status = Column(String(20), default="pending", index=True)
    priority = Column(String(10), default="medium", index=True)
    
    # Timestamps (automatic)
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, onupdate=datetime.utcnow)
    
    # Foreign Key (link to users table)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=True)
    
    # Relationship (optional - for joins)
    # user = relationship("User", back_populates="tasks")
    
    def __repr__(self):
        return f"<Task(id={self.id}, title='{self.title}', status='{self.status}')>"
```

**Key Concepts:**
- âœ… `Column()`: Defines database column
- âœ… `index=True`: Creates database index (faster queries)
- âœ… `ForeignKey()`: Links to another table
- âœ… `default`: Sets default value
- âœ… `onupdate`: Auto-updates timestamp on modification

**Pydantic vs SQLAlchemy:**
| Aspect | Pydantic (task.py) | SQLAlchemy (database.py) |
|--------|-------------------|-------------------------|
| Purpose | API validation | Database schema |
| Used for | Request/Response | Database queries |
| Validates | User input | N/A |
| Storage | Memory only | PostgreSQL |

---

#### **FILE 4: `src/api/routes.py` - API Endpoints**

**Purpose:** HTTP endpoint handlers (CRUD operations)

**What it does:**
```python
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from typing import List

from src.models.task import TaskCreate, TaskUpdate, TaskResponse
from src.models.database import Task
from src.api.dependencies import get_db

router = APIRouter(tags=["tasks"])

@router.post("/tasks", response_model=TaskResponse, status_code=status.HTTP_201_CREATED)
async def create_task(
    task: TaskCreate,              # Request body (validated by Pydantic)
    db: Session = Depends(get_db)  # Database session (injected)
):
    """
    Create a new task
    
    - **title**: Task title (required, 1-200 chars)
    - **description**: Optional description
    - **status**: pending/in_progress/completed/cancelled
    - **priority**: low/medium/high/critical
    """
    # Create SQLAlchemy model from Pydantic model
    db_task = Task(**task.dict())
    
    # Save to database
    db.add(db_task)
    db.commit()
    db.refresh(db_task)  # Get auto-generated ID
    
    return db_task

@router.get("/tasks", response_model=List[TaskResponse])
async def get_tasks(
    skip: int = 0,                  # Pagination: offset
    limit: int = 100,               # Pagination: max items
    status: Optional[str] = None,   # Filter by status
    db: Session = Depends(get_db)
):
    """
    Get all tasks (with pagination and filtering)
    """
    query = db.query(Task)
    
    # Apply filter if provided
    if status:
        query = query.filter(Task.status == status)
    
    # Apply pagination
    tasks = query.offset(skip).limit(limit).all()
    
    return tasks

@router.get("/tasks/{task_id}", response_model=TaskResponse)
async def get_task(
    task_id: int,                   # Path parameter
    db: Session = Depends(get_db)
):
    """
    Get a specific task by ID
    """
    task = db.query(Task).filter(Task.id == task_id).first()
    
    if not task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Task with id {task_id} not found"
        )
    
    return task

@router.put("/tasks/{task_id}", response_model=TaskResponse)
async def update_task(
    task_id: int,
    task_update: TaskUpdate,
    db: Session = Depends(get_db)
):
    """
    Update a task (partial update allowed)
    """
    db_task = db.query(Task).filter(Task.id == task_id).first()
    
    if not db_task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Task with id {task_id} not found"
        )
    
    # Update only provided fields
    update_data = task_update.dict(exclude_unset=True)
    for key, value in update_data.items():
        setattr(db_task, key, value)
    
    db.commit()
    db.refresh(db_task)
    
    return db_task

@router.delete("/tasks/{task_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_task(
    task_id: int,
    db: Session = Depends(get_db)
):
    """
    Delete a task
    """
    db_task = db.query(Task).filter(Task.id == task_id).first()
    
    if not db_task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Task with id {task_id} not found"
        )
    
    db.delete(db_task)
    db.commit()
    
    return None
```

**Key Concepts:**
- âœ… `@router.post()`: Defines HTTP POST endpoint
- âœ… `response_model`: Automatic response serialization
- âœ… `Depends()`: Dependency injection (like Spring Boot)
- âœ… `HTTPException`: Proper HTTP error responses
- âœ… `status.HTTP_*`: Standard HTTP status codes

---

#### **FILE 5: `src/database/connection.py` - Database Setup**

**Purpose:** PostgreSQL connection configuration

**What it does:**
```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from src.config.settings import settings

# Database URL format: postgresql://user:password@host:port/database
DATABASE_URL = f"postgresql://{settings.DB_USER}:{settings.DB_PASSWORD}@{settings.DB_HOST}:{settings.DB_PORT}/{settings.DB_NAME}"

# Create database engine
engine = create_engine(
    DATABASE_URL,
    pool_size=10,          # Connection pool size
    max_overflow=20,       # Additional connections when pool is full
    pool_pre_ping=True,    # Verify connections before using
    echo=settings.DEBUG    # Log SQL queries (dev only)
)

# Session factory
SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine
)
```

**Key Concepts:**
- âœ… `create_engine()`: Creates database connection pool
- âœ… `pool_size`: Number of persistent connections
- âœ… `max_overflow`: Extra connections for traffic spikes
- âœ… `pool_pre_ping`: Prevents "connection lost" errors
- âœ… `sessionmaker()`: Creates database session factory

---

#### **FILE 6: `src/api/dependencies.py` - Dependency Injection**

**Purpose:** Provides database sessions to endpoints

**What it does:**
```python
from sqlalchemy.orm import Session
from src.database.connection import SessionLocal

def get_db():
    """
    Provides database session to API endpoints
    
    Automatically:
    - Creates new session
    - Yields it to endpoint
    - Closes session after request
    """
    db = SessionLocal()
    try:
        yield db  # Endpoint uses this session
    finally:
        db.close()  # Always close (even if error occurs)
```

**Key Concepts:**
- âœ… `yield`: Python generator (gives control to endpoint)
- âœ… `try/finally`: Ensures cleanup happens
- âœ… Automatic session management (no manual close needed)

**How it's used:**
```python
@router.post("/tasks")
async def create_task(db: Session = Depends(get_db)):
    # 'db' is automatically provided and cleaned up
    pass
```

---

#### **FILE 7: `src/config/settings.py` - Configuration**

**Purpose:** Environment variable management

**What it does:**
```python
from pydantic import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """
    Application settings from environment variables
    """
    # Application
    APP_NAME: str = "task-service"
    DEBUG: bool = False
    VERSION: str = "1.0.0"
    
    # Database
    DB_USER: str = "postgres"
    DB_PASSWORD: str = "postgres"
    DB_HOST: str = "localhost"
    DB_PORT: int = 5432
    DB_NAME: str = "taskdb"
    
    # OpenTelemetry
    OTEL_EXPORTER_OTLP_ENDPOINT: Optional[str] = None
    OTEL_SERVICE_NAME: str = "task-service"
    
    # Security
    SECRET_KEY: str = "change-me-in-production"
    
    class Config:
        env_file = ".env"  # Load from .env file
        case_sensitive = True

# Singleton instance
settings = Settings()
```

**Key Concepts:**
- âœ… `BaseSettings`: Pydantic settings management
- âœ… `env_file`: Loads from `.env` file
- âœ… Type validation for env vars
- âœ… Default values provided

**`.env` file example:**
```bash
DEBUG=true
DB_HOST=localhost
DB_PASSWORD=mysecretpassword
```

---

#### **FILE 8: `src/observability/tracing.py` - OpenTelemetry**

**Purpose:** Distributed tracing setup

**What it does:**
```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor

from src.config.settings import settings

def setup_tracing():
    """
    Initialize OpenTelemetry tracing
    
    Automatically traces:
    - HTTP requests
    - Database queries  
    - External API calls
    """
    # Set up trace provider
    trace.set_tracer_provider(TracerProvider())
    
    # Export traces to OpenTelemetry Collector
    otlp_exporter = OTLPSpanExporter(
        endpoint=settings.OTEL_EXPORTER_OTLP_ENDPOINT
    )
    
    # Batch spans for efficiency
    span_processor = BatchSpanProcessor(otlp_exporter)
    trace.get_tracer_provider().add_span_processor(span_processor)
    
    # Auto-instrument SQLAlchemy
    SQLAlchemyInstrumentor().instrument()
    
    print(f"âœ… OpenTelemetry tracing initialized for {settings.OTEL_SERVICE_NAME}")
```

**What you'll see in Jaeger:**
```
Request: POST /api/v1/tasks
  â”œâ”€ Span 1: HTTP Request (200ms)
  â”œâ”€ Span 2: Input Validation (5ms)
  â”œâ”€ Span 3: Database Query (150ms)
  â”‚   â””â”€ Span 4: SQL: INSERT INTO tasks
  â””â”€ Span 5: Response Serialization (10ms)

Total: 365ms
```

---

### **Step 2.4: Testing Files Explained**

#### **FILE 9: `tests/conftest.py` - Pytest Fixtures**

**Purpose:** Shared test setup (database, client, etc.)

**What it does:**
```python
import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from src.main import app
from src.models.database import Base
from src.api.dependencies import get_db

# Test database (SQLite in-memory)
SQLALCHEMY_DATABASE_URL = "sqlite:///./test.db"

engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

@pytest.fixture(scope="function")
def db():
    """
    Provides clean database for each test
    """
    Base.metadata.create_all(bind=engine)
    db = TestingSessionLocal()
    try:
        yield db
    finally:
        db.close()
        Base.metadata.drop_all(bind=engine)

@pytest.fixture(scope="function")
def client(db):
    """
    Provides FastAPI test client
    """
    def override_get_db():
        yield db
    
    app.dependency_overrides[get_db] = override_get_db
    yield TestClient(app)
    app.dependency_overrides.clear()
```

**Key Concepts:**
- âœ… `@pytest.fixture`: Reusable test setup
- âœ… `scope="function"`: Fresh database per test
- âœ… `TestClient`: Simulates HTTP requests
- âœ… `dependency_overrides`: Inject test database

---

#### **FILE 10: `tests/unit/test_routes.py` - Unit Tests**

**What it does:**
```python
def test_create_task(client):
    """Test task creation"""
    response = client.post(
        "/api/v1/tasks",
        json={
            "title": "Test Task",
            "description": "Test description",
            "priority": "high"
        }
    )
    assert response.status_code == 201
    data = response.json()
    assert data["title"] == "Test Task"
    assert data["priority"] == "high"
    assert "id" in data
    assert "created_at" in data

def test_get_tasks(client):
    """Test listing tasks"""
    # Create test tasks
    client.post("/api/v1/tasks", json={"title": "Task 1"})
    client.post("/api/v1/tasks", json={"title": "Task 2"})
    
    # Get all tasks
    response = client.get("/api/v1/tasks")
    assert response.status_code == 200
    tasks = response.json()
    assert len(tasks) == 2

def test_validation_error(client):
    """Test that validation works"""
    response = client.post(
        "/api/v1/tasks",
        json={"title": ""}  # Empty title should fail
    )
    assert response.status_code == 422  # Validation error
```

---

#### **FILE 11: `Dockerfile` - Multi-Stage Build**

**Purpose:** Creates optimized Docker image

**What it does:**
```dockerfile
# ========================================
# Stage 1: Builder (compile dependencies)
# ========================================
FROM python:3.11-slim as builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python packages to user directory
RUN pip install --user --no-cache-dir -r requirements.txt

# ========================================
# Stage 2: Runtime (minimal, production)
# ========================================
FROM python:3.11-slim

WORKDIR /app

# Install runtime dependencies only
RUN apt-get update && apt-get install -y \
    libpq5 \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /root/.local /root/.local

# Copy application code
COPY ./src ./src

# Make Python packages available
ENV PATH=/root/.local/bin:$PATH
ENV PYTHONPATH=/app

# Non-root user (security best practice)
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Key Concepts:**
- âœ… Multi-stage build: Smaller final image
- âœ… Non-root user: Security best practice
- âœ… Health check: Kubernetes readiness
- âœ… Minimal dependencies: Only runtime needs

**Image size comparison:**
- Single-stage: ~1.2GB
- Multi-stage: ~200MB (83% smaller!)

---

#### **FILE 12: `requirements.txt` - Dependencies**

```txt
# Web Framework
fastapi==0.104.1
uvicorn[standard]==0.24.0

# Database
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
alembic==1.12.1

# Validation
pydantic==2.5.0
pydantic-settings==2.1.0

# OpenTelemetry (Observability)
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-fastapi==0.42b0
opentelemetry-instrumentation-sqlalchemy==0.42b0
opentelemetry-exporter-otlp==1.21.0

# Utilities
python-dotenv==1.0.0
```

---

## ðŸ“Š Tomorrow's Session Timeline

| Time | Step | Activity | Duration |
|------|------|----------|----------|
| 00:00 | 2.1 | Create directory structure | 5 min |
| 00:05 | 2.2 | Implement `main.py` | 5 min |
| 00:10 | 2.3 | Create Pydantic models | 10 min |
| 00:20 | 2.4 | Create SQLAlchemy models | 5 min |
| 00:25 | 2.5 | Implement API routes | 10 min |
| 00:35 | 2.6 | Database connection | 5 min |
| 00:40 | 2.7 | Configuration setup | 5 min |
| 00:45 | 2.8 | OpenTelemetry tracing | 5 min |
| 00:50 | 2.9 | Create tests | 10 min |
| 01:00 | 2.10 | Create Dockerfile | 5 min |
| 01:05 | 2.11 | **Test everything!** | 10 min |
| 01:15 | â€” | **Break** | 5 min |
| 01:20 | 2.12-15 | user-service (Flask) | 30 min |
| 01:50 | 2.16-20 | auth-service (JWT) | 30 min |
| 02:20 | 2.21 | notification-service | 15 min |
| 02:35 | â€” | **Review & Questions** | 10 min |

**Total: ~2 hours 45 minutes**

---

## ðŸŽ“ Key Takeaways for Tomorrow

### **Architectural Patterns:**
1. **Layered Architecture**: API â†’ Business Logic â†’ Database
2. **Dependency Injection**: Clean, testable code
3. **12-Factor App**: Configuration via environment variables
4. **Repository Pattern**: Abstracted database access

### **Best Practices:**
1. **Type Hints**: Python 3.11+ type annotations
2. **Validation**: Pydantic models prevent bad data
3. **Error Handling**: Proper HTTP status codes
4. **Testing**: Unit, integration, e2e coverage
5. **Observability**: Trace every request
6. **Security**: Non-root containers, input validation

### **Technologies Mastered:**
- âœ… FastAPI (modern Python web framework)
- âœ… Pydantic (data validation)
- âœ… SQLAlchemy (ORM)
- âœ… OpenTelemetry (observability)
- âœ… Docker (containerization)
- âœ… Pytest (testing)

---

## ðŸ“ Pre-Session Checklist

Review tonight:
- [ ] Understand request flow diagram
- [ ] Read Pydantic vs SQLAlchemy comparison
- [ ] Review Docker multi-stage build concept
- [ ] Understand dependency injection pattern
- [ ] Review OpenTelemetry benefits

---

## â“ Questions to Consider

1. **Why use both Pydantic AND SQLAlchemy models?**
   - Separation of concerns: API layer vs Database layer
   - Pydantic: Validation + serialization
   - SQLAlchemy: Database operations

2. **Why OpenTelemetry over custom logging?**
   - Industry standard
   - Vendor-neutral (works with Jaeger, Zipkin, etc.)
   - Automatic instrumentation
   - Distributed tracing across services

3. **Why multi-stage Docker builds?**
   - Smaller images (faster deployments)
   - Security (no build tools in production)
   - Cost savings (bandwidth, storage)

---

## ðŸš€ Tomorrow's Goals

By end of session, you'll have:
- âœ… 4 production-ready microservices
- âœ… ~2000 lines of production code
- âœ… >85% test coverage
- âœ… Docker images for all services
- âœ… OpenTelemetry instrumentation
- âœ… Complete understanding of each file

---

## ðŸ’¡ Tips for Tomorrow

1. **Ask questions anytime** - I'll explain every line
2. **We'll test as we go** - See it working immediately
3. **Copy-paste is fine** - Understanding > typing speed
4. **We'll fix errors together** - Debugging is learning
5. **Take breaks** - 5 minutes every hour

---

**See you tomorrow! ðŸŽ¯**

Rest well tonight. Tomorrow we build production-grade microservices! ðŸš€

---

*Prepared by: GitHub Copilot*  
*Date: December 9, 2025*  
*Next Session: December 10, 2025*
